---
title: TOS Chat - Custom AI Chatbot
description: Specialized chatbot that allows the user to “chat” with company documents like terms of service and privacy policies. Techstack includes the OpenAI api, LangChain, and Pinecone, an external vector store (for semantic search).
date: "2023-01-03"
cover: "/covers/tos-chat.png"
tags: { "TypeScript": true, "NextJS": true, "TailwindCSS": true, "Pinecone": true, "LangChain": true, "MongoDB": true }
live: "https://www.tos.chat/"
---

## **Introduction**

In today's digital world, we are constantly interacting with various company documents like terms of service and privacy policies. However, these documents are often lengthy, filled with legal jargon, and difficult to understand. To make this information more accessible and understandable, we have developed TOS Chat, a specialized chatbot that allows users to "chat" with these company documents. In this blog post, we will dive into the technical details of the project, which utilizes the OpenAI API, LangChain, Pinecone (an external vector store for semantic search), and an innovative PDF ingestion script.

## **PDF Ingestion**

One of the key challenges in this project was to process the raw text from company documents in a way that allows the chatbot to answer users' questions effectively. To achieve this, we created a PDF ingestion script that performs the following tasks:

1. Load the PDF file.
2. Extract text content from the PDF.
3. Split the raw text into chunks due to the token limit on OpenAI queries.

The token limit for OpenAI queries is 4,000, which includes the context, question, and answer. Thus, it is crucial to keep the raw text within this limit.

## **Creating Embeddings with the OpenAI API**

After splitting the raw text into chunks, we convert these chunks into embeddings using the **`text-embedding-ada-002`** model. The resulting embeddings are vectors with 1,536 dimensions, which are then stored in a Pinecone index.

To ensure that document information doesn't get mixed, we store each document's vectors under their own namespace within the Pinecone index.

## **User Interaction and Question Processing**

Once the documents are ready in the database, the user can ask a question. Their question, along with previous messages in the conversation (if applicable), is handed off to a large language model (LLM) using the OpenAI API. The LLM is instructed to combine the chat history and the current question into one new question, providing TOS Chat with context for the conversation as a whole.

## **Searching the Vector Database and Generating an Answer**

With a revised question, we convert it into an embedding using the **`text-embedding-ada-002`** model again and search the vector database for similar vectors. The similar vectors returned are used as context from the PDF to answer the question.

The source material context is combined with the revised question and passed to the LLM to generate an answer. This answer is then provided to the user, along with the citation used.

## **Hosting Challenges and the Switch to Heroku**

Initially, the TOS Chat website was hosted on Vercel, a popular platform for deploying web applications. However, a significant issue arose during the development process. The API handler, which was running as a serverless function, often faced timeouts due to its 10-second max execution time on Vercel. This limitation proved to be problematic, as it negatively impacted the user experience and the overall performance of the chatbot.

![timeout](/images/tos-chat/timeout.png)

Vercel offers a more advanced feature called Edge Functions that support higher execution times and additional capabilities such as data streams. Unfortunately, Edge Functions are not compatible with Node API functions, which were required for this project. One of the primary reasons behind this incompatibility is that the LangChain VectorStore package relies on the **`fs`** module, which is not supported in Vercel Edge Functions.

To resolve these issues, the hosting platform was switched to Heroku. Heroku offers out-of-the-box support for data streams and Node APIs, making it a perfect fit for the project requirements. By migrating to Heroku, the timeout issues previously faced on Vercel were eliminated, improving the overall
